{
 "cells": [
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import zipfile\n",
    "def download_and_setup_data(url, output_dir):\n",
    "    \"\"\"\n",
    "    Downloads the ZIP file from the specified URL and extracts it to the target directory.\n",
    "    Uses the central print.\n",
    "    \"\"\"\n",
    "    # 1. Create target directory if it doesn't exist\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "        print(f\"Directory created: {output_dir}\")\n",
    "\n",
    "    # Temporary filename for download\n",
    "    temp_zip = \"temp_dataset_download.zip\"\n",
    "\n",
    "    print(f\"Starting download from: {url} ...\")\n",
    "\n",
    "    try:\n",
    "        # 2. Download with streaming\n",
    "        response = requests.get(url, stream=True)\n",
    "        response.raise_for_status() # Raise error if link is unreachable\n",
    "\n",
    "        total_size = 0\n",
    "        with open(temp_zip, \"wb\") as f:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "                    total_size += len(chunk)\n",
    "\n",
    "        size_mb = total_size / (1024 * 1024)\n",
    "        print(f\"Download complete! Size: {size_mb:.2f} MB\")\n",
    "        print(\"Unzipping in progress...\")\n",
    "\n",
    "        # 3. Extract to target directory\n",
    "        with zipfile.ZipFile(temp_zip, 'r') as zip_ref:\n",
    "            zip_ref.extractall(output_dir)\n",
    "\n",
    "        print(f\"Successfully extracted to: {output_dir}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred during download/extraction: {e}\")\n",
    "\n",
    "    finally:\n",
    "        # 4. Cleanup: remove temporary zip\n",
    "        if os.path.exists(temp_zip):\n",
    "            os.remove(temp_zip)\n",
    "            print(\"Temporary files deleted.\")\n"
   ],
   "id": "initial_id"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "link4 = \"https://bmeedu-my.sharepoint.com/:u:/g/personal/gyires-toth_balint_vik_bme_hu/IQAlEFc87da4SLpRVTCs81KwAS3DG4Ft8JPtUKQe9vV5eng?download=1\"\n",
    "download_and_setup_data(link4, '../data')"
   ],
   "id": "aac7127fe827de3b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import glob\n",
    "import re\n",
    "import numpy as np\n",
    "import shutil\n",
    "\n",
    "# --- KONFIGURÁCIÓ ---\n",
    "N_BARS_LOOKBACK = 100\n",
    "\n",
    "if os.path.exists(\"../data\"):\n",
    "    DATA_ROOT = \"../data\"\n",
    "else:\n",
    "    DATA_ROOT = os.path.abspath(\"./Data\")\n",
    "\n",
    "OUTPUT_DIR = DATA_ROOT\n",
    "ALLOWED_ASSETS = ['EURUSD', 'XAU']\n",
    "\n",
    "# ==========================================\n",
    "# 1. SEGÉDFÜGGVÉNYEK\n",
    "# ==========================================\n",
    "\n",
    "def robust_parse_ts(value):\n",
    "    try:\n",
    "        if pd.isna(value) or value == \"\": return pd.NaT\n",
    "        if str(value).isdigit() or isinstance(value, (int, float)):\n",
    "            val_int = int(value)\n",
    "            if val_int > 30000000000: return pd.to_datetime(val_int, unit='ms')\n",
    "            else: return pd.to_datetime(val_int, unit='s')\n",
    "        return pd.to_datetime(value)\n",
    "    except:\n",
    "        return pd.NaT\n",
    "\n",
    "def get_uniform_name(filename):\n",
    "    if not filename: return \"UNKNOWN\"\n",
    "    clean = filename\n",
    "    if '-' in filename: parts = filename.split('-', 1); clean = parts[1] if len(parts)>1 else clean\n",
    "    base = os.path.splitext(clean)[0]\n",
    "    parts = base.split('_')\n",
    "    if len(parts) >= 2:\n",
    "        asset = parts[0]; tf = parts[1]\n",
    "        if re.search(r'\\d+\\s*(minute|min|m|M)$', tf, re.IGNORECASE):\n",
    "            tf = re.sub(r'(minute|min|m|M)$', 'min', tf, flags=re.IGNORECASE)\n",
    "        elif re.search(r'\\d+\\s*(hour|h)$', tf, re.IGNORECASE):\n",
    "            tf = re.sub(r'(hour|h)$', 'H', tf, flags=re.IGNORECASE)\n",
    "        return f\"{asset}_{tf}\"\n",
    "    return base\n",
    "\n",
    "def is_asset_allowed(filename):\n",
    "    fname_upper = filename.upper()\n",
    "    for asset in ALLOWED_ASSETS:\n",
    "        if asset in fname_upper:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# ==========================================\n",
    "# 2. ÜTKÖZÉS KEZELÉS\n",
    "# ==========================================\n",
    "def filter_overlaps(df):\n",
    "    if df.empty: return df\n",
    "\n",
    "    print(\"\\n[SZŰRÉS] Címkék tisztítása (Átfedések)...\")\n",
    "    original_count = len(df)\n",
    "\n",
    "    df['start_round'] = df['flag_start_ts'].dt.round('1min')\n",
    "    df['end_round'] = df['flag_end_ts'].dt.round('1min')\n",
    "\n",
    "    df = df.drop_duplicates(subset=['clean_csv_filename', 'start_round', 'end_round', 'label'], keep='first')\n",
    "\n",
    "    df = df.sort_values(by=['clean_csv_filename', 'flag_start_ts'])\n",
    "    indices_to_drop = []\n",
    "\n",
    "    for filename, group in df.groupby('clean_csv_filename'):\n",
    "        group = group.reset_index()\n",
    "        for i in range(len(group) - 1):\n",
    "            curr = group.iloc[i]\n",
    "            next_row = group.iloc[i+1]\n",
    "\n",
    "            if curr['flag_end_ts'] > next_row['flag_start_ts']:\n",
    "                intersection = min(curr['flag_end_ts'], next_row['flag_end_ts']) - next_row['flag_start_ts']\n",
    "                union = max(curr['flag_end_ts'], next_row['flag_end_ts']) - curr['flag_start_ts']\n",
    "\n",
    "                overlap_ratio = 0\n",
    "                if union.total_seconds() > 0:\n",
    "                    overlap_ratio = intersection.total_seconds() / union.total_seconds()\n",
    "\n",
    "                if overlap_ratio > 0.5:\n",
    "                    indices_to_drop.append(next_row['index'])\n",
    "\n",
    "    if indices_to_drop:\n",
    "        df = df.drop(indices_to_drop)\n",
    "\n",
    "    df = df.drop(columns=['start_round', 'end_round'])\n",
    "    print(f\"    -> Címkék száma: {len(df)} (Eredeti: {original_count})\")\n",
    "    return df\n",
    "\n",
    "# ==========================================\n",
    "# 3. CSV MENTÉS (EGYEDI NÉV GENERÁLÁSSAL)\n",
    "# ==========================================\n",
    "def process_and_save_csvs(data_dir, output_dir):\n",
    "    print(f\"\\n[1. FÁZIS] CSV fájlok szűrése és tisztítása...\")\n",
    "\n",
    "    all_csvs = glob.glob(os.path.join(data_dir, \"**/*.csv\"), recursive=True)\n",
    "    kept_files_map = {}\n",
    "\n",
    "    # Set a létrehozott fájlok követésére, hogy ne írjuk felül őket\n",
    "    created_filenames = set()\n",
    "\n",
    "    for csv_path in all_csvs:\n",
    "        filename = os.path.basename(csv_path)\n",
    "\n",
    "        if not is_asset_allowed(filename):\n",
    "            continue\n",
    "\n",
    "        if filename.startswith(\"clean_\") or \"ground_truth_labels\" in filename:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            with open(csv_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                line = f.readline()\n",
    "            sep = ';' if ';' in line else ','\n",
    "\n",
    "            df = pd.read_csv(csv_path, sep=sep)\n",
    "            df.columns = df.columns.str.lower().str.strip()\n",
    "\n",
    "            t_col = next((c for c in df.columns if 'time' in c or 'date' in c), None)\n",
    "            if t_col is None: t_col = df.columns[0]\n",
    "\n",
    "            vals = pd.to_numeric(df[t_col], errors='coerce')\n",
    "            if vals.notna().mean() > 0.8:\n",
    "                if vals.max() > 300000000000: df['dt'] = pd.to_datetime(vals, unit='ms')\n",
    "                else: df['dt'] = pd.to_datetime(vals, unit='s')\n",
    "            else:\n",
    "                df['dt'] = pd.to_datetime(df[t_col], errors='coerce')\n",
    "\n",
    "            df = df.dropna(subset=['dt']).set_index('dt').sort_index()\n",
    "\n",
    "            required_cols = ['open', 'high', 'low', 'close']\n",
    "            available_cols = [c for c in df.columns if c in required_cols or c == 'volume']\n",
    "\n",
    "            if not all(col in available_cols for col in required_cols):\n",
    "                print(f\"    [SKIP] Hiányzó oszlopok: {filename}\")\n",
    "                continue\n",
    "\n",
    "            df = df[available_cols]\n",
    "            df = df[~df.index.duplicated(keep='first')]\n",
    "\n",
    "            # --- JAVÍTÁS: Egyedi fájlnév generálás ---\n",
    "            base_clean_name = f\"clean_{filename}\"\n",
    "            final_save_name = base_clean_name\n",
    "            save_path = os.path.join(output_dir, final_save_name)\n",
    "\n",
    "            counter = 1\n",
    "            # Ha már létezik ilyen nevű fájl (pl. másik mappából jött), adjunk hozzá számot\n",
    "            while os.path.exists(save_path) or final_save_name in created_filenames:\n",
    "                name_part, ext_part = os.path.splitext(base_clean_name)\n",
    "                final_save_name = f\"{name_part}_dup{counter}{ext_part}\"\n",
    "                save_path = os.path.join(output_dir, final_save_name)\n",
    "                counter += 1\n",
    "\n",
    "            df.to_csv(save_path)\n",
    "            created_filenames.add(final_save_name)\n",
    "\n",
    "            # Mapeljük az EREDETI nevet a GENERÁLT névhez\n",
    "            # (Megjegyzés: ha több azonos nevű eredeti fájl van, a map felülíródik,\n",
    "            # de legalább az adatfájlok fizikailag megmaradnak!)\n",
    "            kept_files_map[filename] = save_path\n",
    "\n",
    "            if counter > 1:\n",
    "                print(f\"    -> Mentve (Átnevezve!): {final_save_name} ({len(df)} sor)\")\n",
    "            else:\n",
    "                print(f\"    -> Mentve: {final_save_name} ({len(df)} sor)\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"    [HIBA] {filename}: {e}\")\n",
    "\n",
    "    return kept_files_map\n",
    "\n",
    "# ==========================================\n",
    "# 4. CÍMKE FELDOLGOZÁS\n",
    "# ==========================================\n",
    "def find_best_pole(label_row, ohlcv_df):\n",
    "    try:\n",
    "        anchor_idx = ohlcv_df.index.get_indexer([label_row['flag_start_ts']], method='nearest')[0]\n",
    "        anchor_bar = ohlcv_df.iloc[anchor_idx]\n",
    "        if abs((anchor_bar.name - label_row['flag_start_ts']).total_seconds()) > 14400: return None\n",
    "    except: return None\n",
    "\n",
    "    best_ts = None; max_slope = -float('inf')\n",
    "    for i in range(1, N_BARS_LOOKBACK + 1):\n",
    "        cand_idx = anchor_idx - i\n",
    "        if cand_idx < 0: break\n",
    "        cand_bar = ohlcv_df.iloc[cand_idx]\n",
    "        p_change = 0.0\n",
    "        if label_row['pattern_type'] == \"BULL_FLAG\": p_change = anchor_bar['high'] - cand_bar['low']\n",
    "        elif label_row['pattern_type'] == \"BEAR_FLAG\": p_change = cand_bar['high'] - anchor_bar['low']\n",
    "\n",
    "        if p_change > 0:\n",
    "            slope = p_change / i\n",
    "            if slope > max_slope: max_slope = slope; best_ts = cand_bar.name\n",
    "    return best_ts\n",
    "\n",
    "def process_labels(data_dir, output_dir, kept_files_map):\n",
    "    print(\"\\n[2. FÁZIS] Címkék feldolgozása...\")\n",
    "    json_files = glob.glob(os.path.join(data_dir, \"**/*.json\"), recursive=True)\n",
    "    final_dataset = []\n",
    "    loaded_dfs = {}\n",
    "\n",
    "    for jpath in json_files:\n",
    "        if 'sample' in jpath or 'consensus' in jpath: continue\n",
    "        try:\n",
    "            with open(jpath, 'r') as f: content = json.load(f)\n",
    "            if isinstance(content, dict): content = [content]\n",
    "\n",
    "            for task in content:\n",
    "                original_filename = task.get('file_upload')\n",
    "                original_filename_base = os.path.basename(original_filename) if original_filename else None\n",
    "\n",
    "                if not original_filename_base: continue\n",
    "\n",
    "                clean_path = kept_files_map.get(original_filename_base)\n",
    "\n",
    "                if not clean_path:\n",
    "                    found = False\n",
    "                    for k, v in kept_files_map.items():\n",
    "                        if k in original_filename_base or original_filename_base in k:\n",
    "                            clean_path = v\n",
    "                            found = True\n",
    "                            break\n",
    "                    if not found: continue\n",
    "\n",
    "                clean_filename = os.path.basename(clean_path)\n",
    "                if clean_filename not in loaded_dfs:\n",
    "                    loaded_dfs[clean_filename] = pd.read_csv(clean_path, index_col=0, parse_dates=True)\n",
    "                ohlcv_df = loaded_dfs[clean_filename]\n",
    "\n",
    "                for ann in task.get('annotations', []):\n",
    "                    for res in ann.get('result', []):\n",
    "                        val = res.get('value', {})\n",
    "                        if val.get('timeserieslabels'):\n",
    "                            lbl = val['timeserieslabels'][0]\n",
    "                            p_type = \"BULL_FLAG\" if \"Bullish\" in lbl else \"BEAR_FLAG\" if \"Bearish\" in lbl else \"UNKNOWN\"\n",
    "                            trend = \"BULL\" if p_type == \"BULL_FLAG\" else \"BEAR\" if p_type == \"BEAR_FLAG\" else \"UNKNOWN\"\n",
    "\n",
    "                            start_ts = robust_parse_ts(val['start'])\n",
    "                            end_ts = robust_parse_ts(val['end'])\n",
    "                            if pd.isna(start_ts): continue\n",
    "\n",
    "                            temp_row = {'flag_start_ts': start_ts, 'pattern_type': p_type}\n",
    "                            pole_ts = find_best_pole(temp_row, ohlcv_df)\n",
    "\n",
    "                            if pole_ts:\n",
    "                                final_dataset.append({\n",
    "                                    \"original_filename\": original_filename_base,\n",
    "                                    \"clean_csv_filename\": clean_filename,\n",
    "                                    \"label\": lbl,\n",
    "                                    \"trend_label\": trend,\n",
    "                                    \"flag_start_ts\": start_ts,\n",
    "                                    \"flag_end_ts\": end_ts,\n",
    "                                    \"pole_start_ts\": pole_ts,\n",
    "                                    \"pattern_type\": p_type\n",
    "                                })\n",
    "        except Exception as e:\n",
    "            print(f\"Hiba a JSON feldolgozáskor ({jpath}): {e}\")\n",
    "\n",
    "    df_result = pd.DataFrame(final_dataset)\n",
    "    df_result = filter_overlaps(df_result)\n",
    "    return df_result\n",
    "\n",
    "# ==========================================\n",
    "# 5. TAKARÍTÁS\n",
    "# ==========================================\n",
    "def cleanup_data_folder(data_dir, kept_files_map):\n",
    "    print(\"\\n[3. FÁZIS] Takarítás...\")\n",
    "\n",
    "    keep_set = set([os.path.abspath(p) for p in kept_files_map.values()])\n",
    "    keep_set.add(os.path.abspath(os.path.join(data_dir, \"ground_truth_labels.csv\")))\n",
    "\n",
    "    all_files = glob.glob(os.path.join(data_dir, \"**/*\"), recursive=True)\n",
    "    deleted_count = 0\n",
    "\n",
    "    for f in all_files:\n",
    "        if os.path.isdir(f): continue\n",
    "        if os.path.abspath(f) not in keep_set:\n",
    "            try:\n",
    "                os.remove(f)\n",
    "                deleted_count += 1\n",
    "            except: pass\n",
    "\n",
    "    for root, dirs, files in os.walk(data_dir, topdown=False):\n",
    "        for name in dirs:\n",
    "            try: os.rmdir(os.path.join(root, name))\n",
    "            except: pass\n",
    "\n",
    "    print(f\"    -> Törölve {deleted_count} fájl.\")\n",
    "    print(f\"    -> Megmaradt adatfájlok: {len(keep_set) - 1}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    kept_files_map = process_and_save_csvs(DATA_ROOT, OUTPUT_DIR)\n",
    "\n",
    "    if kept_files_map:\n",
    "        df_final = process_labels(DATA_ROOT, OUTPUT_DIR, kept_files_map)\n",
    "\n",
    "        out_file = os.path.join(OUTPUT_DIR, \"ground_truth_labels.csv\")\n",
    "        df_final.to_csv(out_file, index=False)\n",
    "        print(f\"\\n[KÉSZ] Ground Truth generálva: {len(df_final)} sor.\")\n",
    "\n",
    "        cleanup_data_folder(DATA_ROOT, kept_files_map)\n",
    "\n",
    "    else:\n",
    "        print(\"HIBA: Nem sikerült egyetlen releváns fájlt sem feldolgozni.\")"
   ],
   "id": "bb4badb05103df9c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import os\n",
    "\n",
    "# --- KONFIGURÁCIÓ ---\n",
    "# Ha a Dockerben fut a Jupyter, akkor az útvonalak:\n",
    "OUTPUT_DIR = './output'\n",
    "LABELS_FILE = os.path.join(OUTPUT_DIR, 'ground_truth_labels.csv')\n",
    "\n",
    "# Melyik fájlt akarod látni? (Csak a név eleje kell, pl \"EURUSD_1min\")\n",
    "TARGET_ASSET = \"EURUSD_1min\"\n",
    "\n",
    "# Melyik időszakot?\n",
    "START_DATE = '2025-08-01'\n",
    "END_DATE = '2025-10-01'\n",
    "\n",
    "def plot_interactive_chart():\n",
    "    # 1. Címkék betöltése\n",
    "    if not os.path.exists(LABELS_FILE):\n",
    "        print(\"HIBA: Nincs ground_truth_labels.csv! Futtasd le a 01_data_processing.py-t.\")\n",
    "        return\n",
    "    labels_df = pd.read_csv(LABELS_FILE)\n",
    "\n",
    "    # 2. A megfelelő CSV megkeresése\n",
    "    # A merged fájlok neve pl: merged_EURUSD_1min.csv\n",
    "    csv_pattern = os.path.join(OUTPUT_DIR, f\"merged_{TARGET_ASSET}*.csv\")\n",
    "    found_files = glob.glob(csv_pattern)\n",
    "\n",
    "    if not found_files:\n",
    "        print(f\"HIBA: Nem találom a merged_{TARGET_ASSET}.csv fájlt az output mappában.\")\n",
    "        return\n",
    "\n",
    "    csv_path = found_files[0]\n",
    "    print(f\"Adatok betöltése innen: {os.path.basename(csv_path)} ...\")\n",
    "\n",
    "    # 3. Adatok betöltése és szűrése\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path, index_col=0, parse_dates=True)\n",
    "        # Szűrés időszakra\n",
    "        mask = (df.index >= START_DATE) & (df.index <= END_DATE)\n",
    "        df_subset = df.loc[mask]\n",
    "\n",
    "        if df_subset.empty:\n",
    "            print(\"Nincs adat ebben az időszakban.\")\n",
    "            return\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Hiba a CSV olvasásakor: {e}\")\n",
    "        return\n",
    "\n",
    "    # 4. Releváns címkék szűrése\n",
    "    # A raw_csv_filename oszlopnak egyeznie kell a TARGET_ASSET-tel (pl. EURUSD_1min)\n",
    "    file_labels = labels_df[labels_df['raw_csv_filename'] == TARGET_ASSET]\n",
    "\n",
    "    # Dátum konverzió és szűrés\n",
    "    file_labels['flag_start_ts'] = pd.to_datetime(file_labels['flag_start_ts'])\n",
    "    file_labels['flag_end_ts'] = pd.to_datetime(file_labels['flag_end_ts'])\n",
    "\n",
    "    visible_labels = file_labels[\n",
    "        (file_labels['flag_start_ts'] >= START_DATE) &\n",
    "        (file_labels['flag_start_ts'] <= END_DATE)\n",
    "    ]\n",
    "\n",
    "    print(f\"Kirajzolás... ({len(visible_labels)} címke látható)\")\n",
    "\n",
    "    # --- PLOTTING (Inline) ---\n",
    "    plt.figure(figsize=(15, 7))\n",
    "\n",
    "    # Árfolyam\n",
    "    plt.plot(df_subset.index, df_subset['close'], label='Close Price', color='#333333', linewidth=1)\n",
    "\n",
    "    # Címkék\n",
    "    for _, row in visible_labels.iterrows():\n",
    "        color = 'green' if 'Bullish' in row['label'] else 'red'\n",
    "        fill_color = 'lightgreen' if 'Bullish' in row['label'] else 'lightcoral'\n",
    "\n",
    "        # Zászló Doboz (Flag)\n",
    "        plt.axvspan(row['flag_start_ts'], row['flag_end_ts'], color=fill_color, alpha=0.3)\n",
    "\n",
    "        # Rúd Vonal (Pole) - Ha sikerült kiszámolni\n",
    "        if pd.notna(row['pole_start_ts']):\n",
    "            pole_start = pd.to_datetime(row['pole_start_ts'])\n",
    "\n",
    "            # Megpróbáljuk megkeresni az árakat a szebb vonalhoz\n",
    "            try:\n",
    "                # Nearest lookup a pandas indexben\n",
    "                p1_idx = df.index.get_indexer([pole_start], method='nearest')[0]\n",
    "                p2_idx = df.index.get_indexer([row['flag_start_ts']], method='nearest')[0]\n",
    "\n",
    "                # Ár kiválasztása (Bullnál Low->High, Bearnél High->Low)\n",
    "                if 'Bullish' in row['label']:\n",
    "                    y1 = df.iloc[p1_idx]['low']  # Rúd alja\n",
    "                    y2 = df.iloc[p2_idx]['high'] # Rúd teteje (Flag kezdete)\n",
    "                else:\n",
    "                    y1 = df.iloc[p1_idx]['high'] # Rúd teteje\n",
    "                    y2 = df.iloc[p2_idx]['low']  # Rúd alja (Flag kezdete)\n",
    "\n",
    "                # Vastag vonal a rúdtól a zászlóig\n",
    "                plt.plot([pole_start, row['flag_start_ts']], [y1, y2],\n",
    "                         color=color, linewidth=2.5, linestyle='-', marker='o')\n",
    "            except:\n",
    "                # Fallback: csak függőleges vonal, ha hiba van az árkeresésnél\n",
    "                plt.axvline(pole_start, color=color, linestyle='--', alpha=0.5)\n",
    "\n",
    "        # Felirat\n",
    "        mid_point = row['flag_start_ts'] + (row['flag_end_ts'] - row['flag_start_ts']) / 2\n",
    "        plt.text(mid_point, df_subset['close'].mean(), row['label'], rotation=90, verticalalignment='center', fontsize=9)\n",
    "\n",
    "    plt.title(f\"{TARGET_ASSET} Elemzés ({START_DATE} - {END_DATE})\", fontsize=14)\n",
    "    plt.xlabel(\"Dátum\")\n",
    "    plt.ylabel(\"Árfolyam\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend(loc='upper left')\n",
    "\n",
    "    # Szép dátum formázás az X tengelyen\n",
    "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "    plt.gcf().autofmt_xdate()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Futtatás\n",
    "plot_interactive_chart()"
   ],
   "id": "d0d5ee79a86ad00e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "2d96500fb4a39f4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "4404fc0567f650e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "1bd889e6bf8c949b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
